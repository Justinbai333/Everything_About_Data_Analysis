### 决策树 Decision Tree

- **原理**：决策树是一种树形结构，每一个内部节点（internal node）代表某个属性上的判断，每一个分支（branch）代表这个属性判断的结果，最后的叶节点（leaf node）代表分类结果。

- **ID3**: 以 **熵(Entropy)** 作为判断谁是父节点的标准，追求熵的最小化。 \
  熵的定义：E(S) = &sum; (-p<sub>i</sub>log<sub>2</sub>p<sub>i</sub>) \
  Information Gain = Entropy(Before) - Entropy(After) \
  问题：熵的最小化会带来过度拟合(Overfitting)，理论上将N个样本分成N个叶节点熵最小，但是没有意义。

- **C4.5**：对ID3的优化，以信息增益率（Gain Ratio）作为判断标准。 \
  Gain Ratio = Information Gain / Split Info \
  添加了对于过度细分的惩罚。

- **CART** (Classification and Regression Tree) \
  CART的分类效果一般优于其他决策树 \
  以**Gini Index**作为优化目标，选取GINI Index小的分叉优先分叉。Regression的部分：对每个叶节点里的数据分析方差，方差小于一定程度时停止分裂。

  Gini = 1 - &sum; (p<sub>i</sub>&sup2;)

- **对于过度拟合的处理**：
  1. Pruning（剪树枝）：如果一次分叉的Gini Index的缩小小于一定值的时候，就不进行这样的分叉
  2. 使用树群（Ensemble）：如Random Forest， Gradient Boost
