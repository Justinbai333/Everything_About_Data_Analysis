### 人工神经网络 Artificial Neural Network

- **原理：** 使用输入层 - 隐藏层 - 输出层的结构，通过调节各层之间的函数权重来达到最优化的模型。ANN分Supervised和Unsupervised两种，Supervised给定期望输出，通过调整权重使模型预测逼近期望输出，Unsupervised给定质量的测量尺度，根据该尺度来优化权重。

- **常见拓扑结构**
  1. 单层前向神经网络
  2. 多层前向神经网络
  3. 反馈网络
  4. 随机神经网络
  5. 竞争神经网络

    [神经网络常见拓扑结构](\神经网络常见拓扑结构.webp)

- **人工神经网络的分类**
  1. 单层感知器模型

    ANN的最早应用，问题；无法处理线性不可分的变量。（Collinearity）
  2. 多层感知器模型

    添加中间层(hidden layer)，调节层之间的连接权重。 \
    问题：随着神经网络的加深，训练结果容易倾向于局部最优解，而偏离全局最优解。

  3. 反向传播BP模型

    多层感知器模型的一种。通过反向传播误差来修改层间权重，可以实现自动优化。

    问题：梯度衰减，反向传播梯度时，每传递一层，梯度衰减为原来的0.25。在层数多的情况下，梯度以指数衰减，低层次几乎收不到有效的训练信号。

  4. 深度神经网络 DNN

    使用ReLU、maxout代替Sigmoid，克服梯度衰减问题。

    引伸：高速公路网络、深度残差学习

  5. 卷积神经网络 Convolutional Neural Network

    通过在上下层中间添加卷积核作为中介**提取特征**，解决学习参数量爆炸的问题。常被应用于图片的学习中，也可以被应用于语音识别。

    卷积神经网络是一种前向神经网络，不会反向传播损失函数。（普通的全连接神经网络也是）

  6. 循环神经网络 RNN

    RNN中，神经元的输出可以在下一个时间点作用于自身，即i层神经元在t时间的输入除了i-1层的输入以外还有t-1时间在i层的输入。

    RNN的深度是时间长度，同样存在梯度弥散问题，可以通过门来解决。


- **补充**

  1. 卷积神经网络的构成

    输入层、卷积层、激活函数、池化层、全连接层

    池化层的作用：对输入的特征图进行压缩；1. 可以缩小图片尺寸，降低计算复杂度。2. 可以压缩特征，提取主要特征，防止过拟合。

    Note: 多层小卷积核替换一层大卷积核，如两层3\*3换一层5\*5，在保证相同感受野的情况下降低计算量。

  2. ReLU：max{0, x}，在非负区间梯度为常数，不存在梯度消失问题。同时可以增加模型的非线性。缺点：会让部分神经元一直处于死亡状态。

  3. Maxout：输出最大的一组激活值，是一个不固定的方程，具有可学习型。是一个分段线性函数，梯度在每一个线段上都是常数，不村在梯度消失问题。

  4. 梯度弥散 / 梯度爆炸：在深层的神经网络中，第L层计算error对其的梯度时，根据链式法则，会要用到l+1层的梯度。在这个过程中，l和l+1层之间的梯度之间的商如果大于1，则梯度以指数级增长，小于1则梯度以指数级缩小。

  5. 网络退化问题：在梯度问题解决的情况下，神经网络可以得到收敛。随着网络深度增加，网络的表现先逐渐变好至饱和，后迅速下降。

  6. 残差网络：旨在解决梯度弥散和网络退化问题，通过优化残差函数F(x) = H(x) - x 来取得恒等映射H(x) = x, 确保添加更多层的时候不会出现网络退化。同时恒等映射可以创造shortcut connections，解决梯度弥散 / 爆炸问题。

  7. 激活函数的定义：上层节点的输出于下层节点的输入之间的函数关系。

  8. 神经网络如何解决过拟合：

    1. Early Stopping：对训练时间增加限制
    2. Weight Decay：对权值进行regularization增加限制
    3. Boosting：训练多个简单的神经网络取其平均预测值
    4. Dropout：直接忽略部分hidden layer中的神经元，进行训练，用梯度下降法更新参数之后再恢复被忽略的部分。第二次忽略另一批，重复步骤。
    5. 增加样本数量
    6. 人工创造noise
