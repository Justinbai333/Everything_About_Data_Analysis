### 梯度提升树GBDT

- **Boosting的概念**

Boosting也是一种Ensemble Method，区别于Bagging的是，Boosting不采用并行的方式，而是在旧的决策树的预测结果的基础上改进，产生新的决策树。本文的核心时GBDT，对于XGBoost和LightGBM仅少许涉猎。

- **Adaboost**
1. 初始化：最初的样本集，每个样本的权重一样（1/N）。一个弱分类器。
2. 训练完后每次逐步修改样本权重，增加旧分类器判断错误的样本权重，减少判断正确的样本权重。修改权重后的样本丢给新的弱分类器。
3. 重复12之后得到一组弱分类器，赋予预测准确率高的分类器更多的决策权重，组成Ensemble

- **Gradient Boost Decision Tree**
1. 初始化：一个弱CART树（选用的特征少），最初的样本集。
2. 把旧的弱CART树的训练残差送给新的CART树训练，产生新的弱CART树，**此处新的CART树拟合的并不是最初样本的y，而是老的树的残差y - f1(x)。** 残差在回归模型中可以是MSE，在分类模型中可以是log loss。
3. 重复12，达到残差的梯度下降。
4. 最终的OOS Ensemble预测方法为：初始弱CART树预测结果 + 残差 + 残差 + ......，由此产生一个强学习器。

```
举例：

假设Ensemble的大小为三棵树

第一棵CART树f1：拟合 f1(x) ≈ y, 残差：y - f1(x)
第二棵CART树f2：拟合 f2(x) ≈ y - f1(x), 残差：y - f1(x) - f2(x)
第三棵CART树f3：拟合 f3(x) ≈ y - f1(x) - f2(x), 残差：y - f1(x) - f2(x) - f3(x)

由此组合成3棵树的强回归器，而强回归器对于新的x'的预测结果为；

y_pred = f1(x') + f2(x') + f3(x')
```
核心理念还是残差的梯度递减，与Logistic Regression相同

- **GBDT中的步长Learning Rate**

步长α决定了模型的学习速度，步长越短，模型的鲁棒性和普适性就越强，代价是训练的时间也就越长。

以下是一个带有步长的GBDT训练过程
```
第一棵CART树f1：拟合 f1(x) ≈ y, 残差：y - αf1(x)
第二棵CART树f2：拟合 f2(x) ≈ y - αf1(x), 残差：y - αf1(x) - αf2(x)
第三棵CART树f3：拟合 f3(x) ≈ y - αf1(x) - αf2(x), 残差：y - αf1(x) - αf2(x) - αf3(x)
```
- **GBDT的过拟合Overfitting**

不同于随机森林，GBDT存在过拟合的可能。当n_estimators也就是树的数量达到一定程度时便会出现过拟合。（随机森林在树的数量达到一定程度以后，多余的树对模型失去优化效果，但是也不会导致过拟合。）

- **GBDT的优缺点**

  优点：
  - 处理分类和回归问题都可以很高效
  - 相比随机森林，准确度更加高

  缺点：
  - 需要谨慎的调参
  - 可能过拟合



- **XGBoost Extreme Gradient Boost Decision Tree**
1. 在GBDT的基础上可以自定义loss function（GBDT固定是残差）
2. 添加了正则项，惩罚模型复杂度
3. 目标函数的二阶泰勒展开：XGBoost运用二阶展开来近似表达损失函数，相当于函数空间中的牛顿法。
4. 列采样：XGBoost采用了随机森林中的做法，每次节点分裂前进行列随机采样。
5. 缺失值处理：XGBoost运用稀疏感知策略处理缺失值，而GBDT没有设计缺失策略。
6. 并行高效：XGBoost的列块设计能有效支持并行运算，提高效率。

关于XGBoost的系统学习可以直接阅读[Tianqi Chen的Paper](https://arxiv.org/pdf/1603.02754.pdf)

- **LightGBM Light Gradient Boosted Machine**

1. 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据
2. 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。
3. 基于Histogram的决策树算法
4. Leaf-wise的叶子生长策略：LightGBM树的生长方式是垂直方向的，其他的算法都是水平方向的，也就是说Light GBM生长的是树的叶子，其他的算法生长的是树的层次。LightGBM选择具有最大误差的树叶进行生长，当生长同样的树叶，生长叶子的算法可以比基于层的算法减少更多的loss。
5. 使用GOSS算法和EFB算法的梯度提升树（GBDT）称之为LightGBM。

    GOSS:

    核心作用：训练集样本采样优化 \
    1）保留梯度较大的样本；\
    2） 对梯度较小的样本进行随机抽样；\
    3）在计算增益时，对梯度较小的样本增加权重系数.

    EFB:

    核心作用：特征抽取，将互斥特征（一个特征值为零,一个特征值不为零）绑定在一起，从而减少特征维度。
6. 容易过拟合，不建议使用在样本量小的数据集上。

关于LightGBM的系统学习可以阅读[这篇归纳](https://blog.csdn.net/qq_42003997/article/details/103683040)和[这篇详解](https://blog.csdn.net/qq_24591139/article/details/100085359)
