### 逻辑回归 Logistic Regression

- **原理：**解决分类问题，运用了回归的方法。本质是将线性回归模型产生的预测值通过一个跃阶函数转化到[0,1]区间。

- **跃阶函数的使用；** \
 Sigmoid Function: sigmoid(z) = 1 / [1 + e^(-z)] \
 相对于step function的优点：\
 1.连续性 \
 2.因为在[0,1]之间，可以理解成是P( y=1 | X )

- **逻辑回归的代价函数 Cost Function：** \
  对数损失函数 (Logarithmic Loss Function): 选择对数损失函数的原因是，如果使用传统回归模型的代价函数，既误差平方和的话，Sigmoid Function的误差平方和会是一个非凸函数，难以通过求导的方式取得极值。

- **最大似然函数 Maximum Likelihood Function：** \
  MLE：最大似然估计，就是利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值(模型已知，参数未知）

  对数最大似然函数：对连乘积取对数，方便求极值

- **用梯度下降法 Gradient Descent 求代价函数的最小值：**
  1. 在微积分中，对多元函数进行求导，把求导后的结果（各个函数的偏导数）以向量的形式表达出来就是梯度。
  2. 从数学的角度来说，梯度的方向就是函数增长最快的方向，反之，梯度的反方向就是函数下降最快的方向。逻辑回归的代价函数是一种非线性的S型函数，所以不能用求导=0的方法求最优解，所以需要用到Gradient Descent。
  3. 求法，首先设置一个 / 一组初始点 x<sub>0</sub>, 然后每次下降减去步长 a（learning rate）乘以梯度(Gradient), 得到 x<sub>1</sub> =  x<sub>0</sub> - a * Gradient
  4. 求出最大似然函数的最大值，得到的权重也就是逻辑回归的最终解。
